# E-commerce Data Pipeline with AWS Glue

A production-ready data engineering project demonstrating ETL pipelines using AWS Glue, PySpark, and Terraform.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           E-COMMERCE DATA PIPELINE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚   ğŸ“¦ DATA SOURCES              ğŸ”„ ETL PIPELINE                 ğŸ“Š CONSUMPTION   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Orders CSV  â”‚â”€â”€â”€â”€â–¶â”‚  BRONZE LAYER (Raw Data)        â”‚    â”‚   Athena   â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  - Ingest as-is                 â”‚    â”‚  Queries   â”‚   â”‚
â”‚                        â”‚  - Add metadata                  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  - Partition by date            â”‚           â”‚         â”‚
â”‚   â”‚ Products CSV â”‚â”€â”€â”€â”€â–¶â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚                               â”‚         â”‚
â”‚                                      â–¼                               â”‚         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚         â”‚
â”‚   â”‚ Customers CSVâ”‚â”€â”€â”€â”€â–¶â”‚  SILVER LAYER (Cleaned Data)    â”‚           â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  - Data validation              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚
â”‚                        â”‚  - Type casting                 â”‚           â”‚         â”‚
â”‚                        â”‚  - Deduplication                â”‚           â”‚         â”‚
â”‚                        â”‚  - Null handling                â”‚           â”‚         â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚         â”‚
â”‚                                      â”‚                               â”‚         â”‚
â”‚                                      â–¼                               â”‚         â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚         â”‚
â”‚                        â”‚  GOLD LAYER (Business Data)     â”‚           â”‚         â”‚
â”‚                        â”‚  - Aggregations                 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                        â”‚  - KPIs & Metrics               â”‚                     â”‚
â”‚                        â”‚  - Star Schema (Facts & Dims)   â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â–¶â”‚ Dashboards â”‚   â”‚
â”‚                                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
data_engineering_project_ecommerce/
â”‚
â”œâ”€â”€ data/                          # Sample data for local testing
â”‚   â”œâ”€â”€ raw/                       # Raw CSV files
â”‚   â”‚   â”œâ”€â”€ orders.csv
â”‚   â”‚   â”œâ”€â”€ products.csv
â”‚   â”‚   â””â”€â”€ customers.csv
â”‚   â””â”€â”€ generated/                 # Output from data generator
â”‚
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ data_generator/            # Scripts to generate sample data
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ generator.py           # Main data generation logic
â”‚   â”‚   â””â”€â”€ schemas.py             # Data schemas/models
â”‚   â”‚
â”‚   â”œâ”€â”€ glue_jobs/                 # AWS Glue ETL scripts
â”‚   â”‚   â”œâ”€â”€ bronze/                # Raw data ingestion
â”‚   â”‚   â”‚   â””â”€â”€ ingest_raw_data.py
â”‚   â”‚   â”œâ”€â”€ silver/                # Data cleaning & transformation
â”‚   â”‚   â”‚   â””â”€â”€ transform_to_silver.py
â”‚   â”‚   â””â”€â”€ gold/                  # Business aggregations
â”‚   â”‚       â”œâ”€â”€ fact_sales.py
â”‚   â”‚       â””â”€â”€ dim_tables.py
â”‚   â”‚
â”‚   â”œâ”€â”€ quality/                   # Data quality checks
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ validators.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/                     # Utility functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ spark_utils.py
â”‚       â””â”€â”€ s3_utils.py
â”‚
â”œâ”€â”€ terraform/                     # Infrastructure as Code
â”‚   â”œâ”€â”€ main.tf                    # Main Terraform configuration
â”‚   â”œâ”€â”€ variables.tf               # Input variables
â”‚   â”œâ”€â”€ outputs.tf                 # Output values
â”‚   â”œâ”€â”€ s3.tf                      # S3 bucket definitions
â”‚   â”œâ”€â”€ glue.tf                    # Glue resources
â”‚   â”œâ”€â”€ iam.tf                     # IAM roles and policies
â”‚   â””â”€â”€ terraform.tfvars.example   # Example variable values
â”‚
â”œâ”€â”€ tests/                         # Unit and integration tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_data_generator.py
â”‚   â”œâ”€â”€ test_transformations.py
â”‚   â””â”€â”€ conftest.py                # Pytest fixtures
â”‚
â”œâ”€â”€ config/                        # Configuration files
â”‚   â”œâ”€â”€ glue_job_config.json       # Glue job parameters
â”‚   â””â”€â”€ pipeline_config.yaml       # Pipeline configuration
â”‚
â”œâ”€â”€ scripts/                       # Utility scripts
â”‚   â”œâ”€â”€ deploy.sh                  # Deployment script
â”‚   â”œâ”€â”€ run_local.py               # Run ETL locally
â”‚   â””â”€â”€ upload_to_s3.py            # Upload data to S3
â”‚
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ requirements-dev.txt           # Development dependencies
â”œâ”€â”€ .gitignore                     # Git ignore rules
â””â”€â”€ README.md                      # This file
```

## ğŸ¯ Medallion Architecture Explained

### Bronze Layer (Raw)
- **Purpose**: Store data exactly as received from source systems
- **Transformations**: Minimal - only add metadata (ingestion timestamp, source file)
- **Format**: Parquet, partitioned by ingestion date
- **Use Case**: Data lineage, reprocessing, debugging

### Silver Layer (Cleaned)
- **Purpose**: Cleaned, validated, and standardized data
- **Transformations**: 
  - Data type casting
  - Null handling
  - Deduplication
  - Schema enforcement
- **Format**: Parquet, partitioned by business date
- **Use Case**: Ad-hoc analysis, ML feature engineering

### Gold Layer (Business)
- **Purpose**: Business-ready aggregations and metrics
- **Transformations**:
  - Joins across tables
  - Aggregations (daily, weekly, monthly)
  - KPI calculations
- **Format**: Parquet, optimized for query patterns
- **Use Case**: Dashboards, reports, business decisions

## ğŸš€ Getting Started

### Prerequisites
- Python 3.9+
- AWS CLI configured with appropriate credentials
- Terraform 1.0+
- Docker (optional, for local Glue development)

### Local Development

1. **Clone and setup environment**
```bash
git clone <repository-url>
cd data_engineering_project_ecommerce

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt
```

2. **Generate sample data**
```bash
python -m src.data_generator.generator --output data/raw --records 10000
```

3. **Run ETL locally**
```bash
python scripts/run_local.py --layer bronze
python scripts/run_local.py --layer silver
python scripts/run_local.py --layer gold
```

### AWS Deployment

1. **Initialize Terraform**
```bash
cd terraform
terraform init
```

2. **Review and apply infrastructure**
```bash
terraform plan
terraform apply
```

3. **Upload data to S3**
```bash
python scripts/upload_to_s3.py --bucket your-bucket-name --source data/raw
```

4. **Trigger Glue jobs**
```bash
aws glue start-job-run --job-name ecommerce-bronze-ingestion
```

## ğŸ“Š Data Model

### Source Tables

| Table | Description | Key Fields |
|-------|-------------|------------|
| `orders` | Customer orders | order_id, customer_id, order_date, total_amount |
| `products` | Product catalog | product_id, name, category, price |
| `customers` | Customer information | customer_id, email, country, created_at |
| `order_items` | Order line items | order_id, product_id, quantity, unit_price |

### Gold Layer - Star Schema

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   dim_date       â”‚
                    â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
                    â”‚   date_key (PK)  â”‚
                    â”‚   date           â”‚
                    â”‚   day_of_week    â”‚
                    â”‚   month          â”‚
                    â”‚   quarter        â”‚
                    â”‚   year           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dim_customer   â”‚         â”‚         â”‚   dim_product    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚         â”‚         â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚   customer_key   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   product_key    â”‚
â”‚   customer_id    â”‚         â”‚         â”‚   product_id     â”‚
â”‚   email          â”‚         â”‚         â”‚   name           â”‚
â”‚   country        â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”    â”‚   category       â”‚
â”‚   segment        â”‚    â”‚         â”‚    â”‚   price          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  fact_  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚  sales  â”‚
                        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                        â”‚sale_key â”‚
                        â”‚date_key â”‚
                        â”‚customer_â”‚
                        â”‚  key    â”‚
                        â”‚product_ â”‚
                        â”‚  key    â”‚
                        â”‚quantity â”‚
                        â”‚revenue  â”‚
                        â”‚discount â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run specific test file
pytest tests/test_transformations.py -v
```

## ğŸ“ˆ Key Metrics Generated

| Metric | Description | Grain |
|--------|-------------|-------|
| `daily_revenue` | Total revenue per day | Daily |
| `orders_per_customer` | Average orders per customer | Customer |
| `top_products` | Best selling products | Product |
| `customer_lifetime_value` | Total spend per customer | Customer |
| `category_performance` | Revenue by category | Category/Month |

## ğŸ” Security Considerations

- S3 buckets are encrypted at rest (SSE-S3)
- Glue jobs use IAM roles with least-privilege access
- No hardcoded credentials - use AWS Secrets Manager
- VPC endpoints for private connectivity (optional)

## ğŸ“š Learning Resources

- [AWS Glue Documentation](https://docs.aws.amazon.com/glue/)
- [PySpark Guide](https://spark.apache.org/docs/latest/api/python/)
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)
## ğŸ› ï¸ Local Development Notes

**Windows users:** PySpark requires Hadoop binaries (`winutils.exe`). The ETL code is designed for AWS Glue (Linux-based) where this isn't needed. To run locally on Windows, either:
- Set up WSL (Windows Subsystem for Linux)
- Download winutils.exe and set `HADOOP_HOME`

The data generator works on all platforms without additional setup.
## ğŸ“ License

MIT License - feel free to use this project for learning and development.
